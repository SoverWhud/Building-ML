{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация (Clustering) - поиск взаимосвязанных сообщений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея набор обучающих образцов (training data items), которым уже сопоставлены классы, можно обучить модель и затем воспользоваться ей для классификации новых образцов (future data items). Мы назвали этот процесс __обучением с учителем (learning was guided by a teacher)__. Например, роль учителя может сводится к правильной классификации примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь допустим, что мы не располагаем метками (labels), с помощью которых можно было бы обучить модель классификации, например, потому что разметка обошлась бы слишком дорого. Что, если единственный способ получить миллионы меток - попросить, чтобы их вручную проставил человек? Можно попытаться найти какие-то __закономерности в самих данных__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим __вопросно-ответный сайт (question and answer website)__. Когда пользователь будет искать на нашем сайте какую-то информацию, поисковая система, скорее всего, сразу покажет нужный ему ответ. Если имеющиеся ответы пользователя не устраивают, то сайт должен хотя бы показать близкие ответы (related answers), чтобы пользователь быстро понял, какие ответы существуют, и не ушел с сайта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Наивный подход__ - просто взять сообщение, вычислить его схожесть (similarity) со всеми остальными сообщениями и показать первые $n$ самых похожих сообщений в виде ссылок. Но очень скоро такое решение станет слишком накладным. Нужен метод, который быстро находит все взаимосвязанные сообщения (related posts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для достижения этой цели мы воспользуемся __кластеризацией__. Это метод такой организации данных, когда __похожие элементы (similar items) оказываются в одном кластере, а непохожие (dissimilar) - в разных__. \n",
    "* Первая проблема - как превратить текст в нечто такое, для чего можно вычислить сходство (similarity). \n",
    "* Располагая способом измерения похожести, можно с его помощью быстро построить кластер, содержащий похожие сообщения. \n",
    "* И останется только проверить, какие еще документы принадлежат этому кластеру. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Измерение сходства сообщений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения машинного обучения \"сырой\" текст абсолютно бесполезен. Лишь преобразовав его в осмысленные числа, мы сможем подать их на вход алгоритмам машинного обучения, например, кластеризации. Это относится и к более приземленным операциям с текстом, в частности, измерению сходства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Как не надо делать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из мер сходства является __расстояние Левенштейна (Levenshtein distance)__, или __редакционное расстояние (Edit Distance)__. Пусть есть два слова: \"machine\" и \"mchiene\". Их сходство можно определить, как __минимальное число операций редактирования, необходимых для перехода от одного слова к другому__. В данном случае нужно всего две операции: добавить \"a\" после \"m\" и удалить первое \"е\". Однако это весьма дорогой алгоритм, потому что время его работы определяется произведением длин обоих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаясь к сообщениям, мы могли бы схитрить: рассматривать слова целиком как символы и выполнять операции редактирования на уровне слов. Пусть есть два сообщения (для простоты ограничимся только заголовками): \"How to format my hard disk\" (\"Как мне отформатировать жесткий диск\") и \"Hard disk format problems\" (\"Проблемы с форматированием жесткого диска\"). Редакционное расстояние между ними равно 5, потому что нужно удалить слова \"how\", \"to\", \"format\", \"my\", а затем добавить в конец слова \"format\" и \"problems\". Следовательно, можно было бы определить различие между двумя сообщениями, как __количество слов, которые следует добавить или удалить, чтобы преобразовать одни текст в другой__. Эту идею можно было бы немного усовершенствовать, но по существу временная сложность остается той же самой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но даже если бы мы могли добиться достаточного быстродействия, существует еще одна проблема. В нашем примере слово \"format\" дает вклад 2 в расстояние, потому что мы сначала удалили его, а потом снова добавили. Следовательно, такое __расстояние неустойчиво (not robust) относительно изменения порядка слов__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Как надо делать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более надежное (robust) редакционное расстояние дает так называемый __набор слов (bag of word)__. При таком подходе порядок слов полностью игнорируется, а в основу кладутся просто счетчики вхождений слов. Каждому встречающемуся в сообщении слову сопоставляется количество его вхождений, и эти пары сохраняются в векторе. Неудивительно, что эта операция называется __векторизацией (vectorization)__. Обычно вектор получается очень большим, потому что содержит столько элементов, сколько есть слов во всем наборе данных. Возьмем, к примеру, два сообщения с такими счетчиками слов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/two-example-posts.jpg\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбцы __\"Occurrences in post 1\"__ (Вхождений в сообщение 1) и __\"Occurrences in post 2\"__ можно рассматривать как простые векторы. Можно вычислить евклидово расстояние между вектором вопроса и векторами всех сообщений и взять, ближайшее сообщение (правда, как мы уже выяснили, это слишком медленно). А, кроме того, мы можем использовать их как векторы признаков на этапе кластеризации, применяя следующую процедуру:\n",
    "1. Выделить характерные признаки из каждого сообщения и хранить его как вектор сообщений (vector per post).\n",
    "2. Произвести кластеризацию этих векторов.\n",
    "3. Определить кластер для сообщения (post) в вопросе.\n",
    "4. Выбрать из этого кластера сколько-то сообщений, имеющих различное сходство с постом в вопросе. Это повышает\n",
    "разнообразие (diversity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Количество общих слов как мера сходства (similarity measured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование простого текста в набор слов (bag of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нет нужды писать свой код подсчета слов и представления набора слов в виде вектора. Метод __CountVectorizer__ из библиотеки SciKit не только умеет делать это эффективно, но и обладает очень удобным интерфейсом. Функции и классы из SciКit импортируются посредством пакета __sklearn__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр __min_df (минимальная частота в документе)__ определяет, как CountVectorizer должен обходиться с редко встречающимися словами. Если его значением является __целое число__, то слова с меньшим числом вхождений, отбрасываются. Если значение - __дробное число__, то отбрасываются слова, доля которых во всем документе меньше этого числа. Параметр max_df интерпретируется аналогично. Распечатав объект, мы увидим все остальные параметры, которым SciKit присвоила значения по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и следовало ожидать, подсчитываются именно слова (__analyzer=word__), а что считать словом, определяется регулярным выражением __token_pattern__. Например, строка \"cross-validated\" будет разбита на два слова: \"cross\" и \"validated\". Пока не будем обращать внимания на прочие параметры и рассмотрим две строки из нашего примера, содержащие темы сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот список строк можно подать на вход метода векторизатора __fit_transform()__, который и проделает всю работу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизатор распознал семь слов, для каждого из которых мы можем получить счетчики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Это означает__, что первое предложение содержит все слова, кроме \"problems\", а второе - все слова, кроме \"how\", \"my\" и \"to\". На самом деле, это как раз те столбцы, которые присутствовали в предыдущей таблице. Из $X$ мы можем выделить вектор признаков, которым воспользуемся для сравнения документов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала __применим наивный подход__, чтобы показать проблемы, с которыми придется столкнуться на этапе предварительной обработки. Случайно выберем какое-то сообщение и создадим для него вектор счетчиков. Затем вычислим расстояния от него до всех векторов счетчиков и выберем сообщение, для которого расстояние минимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экспериментов возьмем игрушечный набор данных, содержащий такие сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/toy-dataset-posts.jpg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом наборе мы хотим найти сообщение, которое больше других похоже на сообщение \"__imaging databases__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предполагаем, что сообщения находятся в каталоге __TOY_DIR__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases save images permanently.\\n',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "TOY_DIR = \"data/toy\"\n",
    "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задействуем для этой цели __CountVectorizer__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно уведомить векторизатор о полном наборе данных, чтобы он заранее знал, каких слов ожидать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось __5 сообщений__ и __25 различных слов__ всё правильно. Подсчитаны следующие выделенные из текста слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'this', u'toy']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторизуем новое сообщение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что метод __transform__ возвращает разреженные векторы счетчиков, то есть в векторе не хранятся счетчики для каждого слова, потому что большая их часть равна нулю (в сообщении такое слово не встречается). Вместо этого используется потребляющая меньше памяти структура данных __coo_matrix__ (от слова \"COOrdinate\"). Так, для нашего сообщения вектор содержит всего два элемента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользовавшись методом __toarray()__, мы можем восстановить весь массив ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь массив нам понадобится, если мы захотим использовать вектор для вычисления сходства (similarity). __Чтобы измерить сходство__ (при наивном подходе), мы вычисляем евклидово расстояние между векторами счетчиков нового и всех старых сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(vl, v2):\n",
    "    delta = vl - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод __norm()__ вычисляет евклидову норму (кратчайшее расстояние). Это самая очевидная метрика, но есть и много других определений расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почитайте статью __\"Distance Coefficients between Two Lists or Sets\"__ на сайте __The Python Papers Sоurсе Codes__\", где Марис Линь (Maurice Ling) описывает 35 разных расстояний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея функцию __dist_raw__, мы теперь должны перебрать, все сообщения и запомнить самое близкое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_post(X_train, new_post_vec, dist):\n",
    "    best_dist = sys.maxint\n",
    "    best_i = None\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "\n",
    "        if post == new_post:\n",
    "            continue\n",
    "\n",
    "        post_vec = X_train.getrow(i)\n",
    "        d = dist(post_vec, new_post_vec)\n",
    "\n",
    "        print(\"=== Post %i with dist=%.2f: %s\\n\"%(i, d, post))\n",
    "\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print (\"Best post is %i with dist=%.2f\"% (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "\n",
      "\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "print(\"=== New Post: %s\\n\" % new_post)\n",
    "find_best_post(X_train, new_post_vec, dist_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Post 0__ сильнее всеrо отличается от __new_post__, т.к. в них нет ни одного общего слова.\n",
    "* __Post 1__ очень похоже на __new_post__, но не является лучшим, т. к. содержит на одно отсутствующее в новом сообщение слово больше, чем __Post3__.\n",
    "* __Post 4__ - это __Post 3__, повторенное трижды. Поэтому eгo сходство с новым сообщением должно быть точно таким же, как у __Post 3__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распечатка соответствующих векторов признаков объясняет, почему это не так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, одних лишь счетчиков слов недостаточно. Необходимо нормировать векторы на единичную длину (unit length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормировка векторов счетчиков слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функции __dist_norm__ мы будем вычислять расстояние не между исходными, а между нормированными векторами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(vl, v2):\n",
    "    vl_normalized = vl / sp.linalg.norm(vl.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = vl_normalized - v2_normalized\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда результаты измерения сходства изменятся следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"=== New Post: %s\\n\" % new_post)\n",
    "find_best_post(X_train, new_post_vec, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сходство Post 3 и 4 в точности одинаково. C точки зрения подсчета слов в сообщениях этот результат представляется правильным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление малозначимых (less important) слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем еще раз на Post 2. В нем встречаются следующие слова, отсутствующие в новом сообщении: \"most\" (большинство), \"save\" (сохранять), \"images\" (изображения) и \"permanently\" (постоянно). Но их значимость в сообщении совершенно различна. Слова типа \"most\", встречающиеся в самых разных контекстах, называются __стоп-словами (stop words)__. Они несут мало информации и потому должны весить меньше слов типа \"images\", которые встречаются отнюдь не во всех контекстах. Лучше всего вообще удалить слова, которые употребляются настолько широко, что не помогают выявить различия между текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот шаг весьма типичен для обработки текста, поэтому в __CountVectorizer__ для него предусмотрен специальный параметр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы точно знаете, какие стоп-слова хотели бы удалить, то можете передать их полный список. Если параметр __stop_words=english__, то список будет состоять из __318 английских слов__. Каких именно, покажет метод __get_stop_words()__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Новый список__ содержит на семь слов меньше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'toy']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__После исключения стоп-слов__ получаем такие результаты измерения сходства (similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(\"=== New Post: %s\\n\" % new_post)\n",
    "\n",
    "find_best_post(X_train, new_post_vec, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Теперь Post 1 и 2 сравнялись__. Но расстояния изменились несущественно, потому что наши демонстрационные сообщения очень короткие. Картина будет совершенно другой, если взять реальные тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг (Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но одну вещь мы упустили. Одно и то же слово в разных грамматических формах мы считаем разными словами. Например, в Post 2 есть слова \"imaging\" и \"images\". Имеет смысл считать их, как одно слово, ведь они обозначают одно и то же понятие. Нам нужна функция, которая __производит стемминг, то есть выделяет из слова его основу__. В библиотеке SciКit стеммера нет. Но можно скачать бесплатную библиотеку __Natural Language Toolkit__ (NLTK), где имеется стеммер, который легко подключить к countVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В NLTK есть несколько стеммеров. И это необходимо, потому что в каждом языке свои правила стемминга. Для английского языка возьмем класс __SnowballStemmer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphic'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'imag'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'imag'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'imagin'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'imagin'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem ( \"imagine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что результатом стемминга вовсе необязательно являются допустимые английские слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стеммер работает и для глаголов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'buy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'buy'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев, но не всегда:bought - форма прошедшего времени неправильного глагола buy(покупать). Как\n",
    "видим, в этом случае стемммер ошибается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'bought'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bought - форма прошедшего времени неправильного глагола buy(покупать). Как видим, __в этом случае стемммер ошибается__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Совместное использование векторизатора и стеммера из библиотеки NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно произвести стемминг сообщений перед их подачей классу __CountVectorizer__. В этом классе __есть несколько точек подключения__, позволяющих настроить этапы предварительной обработки и лексического анализа. __Препроцессор (preprocessor) и лексический анализатор (tokenizer)__ могут быть переданы конструктору в качестве параметров. Мы не хотим помещать стеммер ни туда, ни сюда, потому что тогда нам пришлось бы заниматься лексическим анализом и нормировкой самостоятельно. \n",
    "\n",
    "Вместо этого мы __переопределим метод build_analyzer__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом каждое сообщение будет подвергнуто следующей обработке:\n",
    "1. Сначала __на шаге предварительной обработки (preprocessing step)__ (в родительском классе) все буквы сообщения будут переведены в нижний регистр.\n",
    "2. __На шаге лексического анализа (tokenization step)__ выделяются отдельные слова (в родительском классе).\n",
    "3. И в завершение из каждого слова будет выделена основа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате у нас получится на один признак меньше, потому что слова __\"images\" и \"imaging\" сольются__. Останется такой список признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если после объединения слов \"images\" и \"imaging\" прогнать новый векторнзатор со стеммингом для всех сообщений, то выяснится, что теперь __на новое сообщение больше всего похоже Post 2 поскольку оно дважды содержит понятие \"imag\"__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(\"=== New Post: %s\\n\" % new_post)\n",
    "\n",
    "find_best_post(X_train, new_post_vec, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Развитие концепции стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть разумный способ построить компактный вектор по зашумленному текстовому сообщению (noisy textual post), вернемся назад и подумаем, в чем на самом деле смысл значений признаков.\n",
    "\n",
    "__Значения признаков (feature values)__ - это просто счетчики вхождения термов (terms) в сообщение. Мы предполагали, что __чем больше это значение, тем важнее терм для данного сообщения__. \n",
    "\n",
    "Но как быть, например, со словом \"subject\" (тема), которое естественно встречается в каждом сообщении? Можно, конечно, попросить countVectorizer удалить его, воспользовавшись параметром max_df. Например, если задать для него значение 0.9, то слова, встречающнеся в 90 и более процентах сообщений, будут игнорироваться. А если слово встречается в 89 процентах сообщений? __Как выбрать правильную величину max_df?__ Проблема в том, что какое бы значение ни выбрать, всегда какие-то термы будут важнее для различения (discriminative) документов, чем другие.\n",
    "\n",
    "__Решить эту проблему можно только одним способом__ - подсчитав частоты термов для каждого сообщения и \"оштрафовав\" те, которые встречаются во многих сообщениях. Иными словами, мы хотим, чтобы данному терму в данном сообщении было сопоставлено большое значение, если он встречается в этом сообщении и мало где еще.\n",
    "\n",
    "Именно в этом состоит смысл характеристики __\"частота терма - обратная частота документа\" (term frequency - inverse document frequency,__ или __TF-IDF)__. Здесь __TF__ относится к подсчету, а __IDF__ - к штрафу. Наивная реализация могла бы выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Мы не просто подсчитали термы, но и нормировали счетчики на длину документа__. Поэтому длинные документы не получат несправедливого преимущества перед короткими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если взять показанный ннже список __D__ уже разбитых на лексемы документов, то мы увидим, что __термы обрабатываются по-разному, хотя в каждом документе встречаются с одинаковой частотой__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.270310072072\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.135155036036\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что __терм a__ не значим ни для одного документа, потому что встречается во всех. __Терм b__ важнее для документа __abb__, чем для __abc__, потому что встречается там дважды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике граничных случаев (corner cases) больше, чем показано в этом примере. Но благодаря SciKit мы можем о них не думать, потому что все они учтены в классе __TfidfVectorizer__, наследующем CountVectorizer. Разумеется, не нужно забывать про наш стеммер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english', decode_error='ignore')\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторы документов вообще не содержат счетчиков. А __содержат они значения TF-IDF для каждого терма.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== New Post: imaging databases\n",
      "\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 2 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(\"=== New Post: %s\\n\" % new_post)\n",
    "\n",
    "find_best_post(X_train, new_post_vec, dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чего мы достигли и к чему стремимся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока что этап предварительной обработки (text pre-processing) включает следующие шаги:\n",
    "1. Лексический анализ текста и разбиение его на лексемы (tokenizing).\n",
    "2. Отбрасывание слов, которые встречаются слишком часто и потому не помогают находить релевантные сообщения.\n",
    "3. Отбрасывание слов, которые встречаются так редко, что вряд ли встретятся в будущих сообщениях.\n",
    "4. Подсчет оставшихся слов.\n",
    "5. Вычисление TF-IDF по счетчикам с учетом всего корпуса текстов (text corpus).\n",
    "Этот процесс позволяет преобразовать исходный зашумленный текст в компактное представление в виде значений признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но при всей простоте и эффективности подхода на основе набора слов с дополнительными расширениями у него имеется ряд недостатков, о которых следует знать.\n",
    "* __Не учитываются связи между словами.__ Если принять описанный подход к векторизации, то у фраз \"Car hits wall\" (Машина врезалась в стену) и \"Wall hits саr\" (Стена врезалась в машину) будет один и тот же набор признаков.\n",
    "* __Не улавливается отрицание.__ Например, фразы \"I will eat ice cream\" (Я стану есть мороженое) и \"I will not eat ice сrеam\" (Я не стану есть мороженое) с точки зрения векторов признаков очень похожи, но имеют противоположный смысл. Впрочем, эту проблему легко решить, если подсчитывать не только отдельные слова (__униграммы__), но также пары слов (__биграммы__) и тройки слов (__триграммы__).\n",
    "* __Никак не обрабатываются ошибки в правописании__. Хотя человеку совершенно понятно, что слова \"database\" и \"databas\" означают одно и то же, в принятом подходе они считаются различными."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
